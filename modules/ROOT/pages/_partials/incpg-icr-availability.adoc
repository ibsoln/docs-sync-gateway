// = High-Availability Replication

// tag::in-this-section[]
*_In this section_*: <<node-distribution>>  | <<configuration-requirements>>  |  <<examples-of-expected-behavior>>  |  <<monitoring-node-distribution>>
// end::in-this-section[]

include::partial$block-caveats.adoc[tag=enterprise-only, subs=attributes]

== Introduction

// tag::overview[]
Sync Gateway's SGR-2-based replications offer built-in High Availability (HA) support.
_Node distribution_ ensures all running replications are evenly distributed across all available nodes, regardless of their originating node.

Any given replication runs on only one node at any give time.

When a node fails, the system automatically resumes the node's replications on an alternative node (providing the replication has been configured on multiple nodes).

_Action_: To make a replication a candidate for HA the user must configure the same replication on at least two Sync Gateway nodes.

:param-msg: Although high-availability replication is not automatic, you can make replication availability more robust by configuring or creating the same replication on multiple nodes. This means that should one node fail the duplicate replication on other nodes may be able to continue.
include::partial$block-caveats.adoc[tag=ce-only]

// end::overview[]

== Node Distribution

The goal of node distribution is to maintain an optimal balance of replications across the cluster.

To achieve this Sync Gateway automatically balances, as equally as possible, the number of replications running on each node.

Where multiple replications are configured on multiple nodes, Sync Gateway automatically distributes these replications across all the available nodes for which the replications are configured.
It continually monitors and redistributes replications as the number of available nodes and the number of running replications in a cluster changes.

The nodes' processing load and bandwidth usage is minimized by ensuring that a replicator runs on only one node at any given time -- even where it has been configured to run on multiple nodes.
This avoids the redundant exchange of data arising from duplicate replication.

// Sync Gateway selects the node to run a replication on is selected by an election process. It uses a heartbeat check to establish whether a node is available or not.
// If it isn't it will try the next node in the configured set.

// Monitoring the node heartbeats is also the trigger for the re-balancing of running replications as it

== Configuration Requirements

To configure replications to be highly available requires that you include their database and replication definitions in the sync gateway configuration on each node within the cluster that you want them to be able to run on.
_At least two nodes are required._

Alternatively, you can use the Admin REST API to initialize the replications on each of the required nodes.

SGW will automatically elect an appropriate node to run them on.
It will redistribute the replication if the node fails, or to maintain an optimal distribution of replications cross available nodes.

:param-msg: As above, submit the same database and replication definitions in the sync gateway configuration (or REST API) on multiple nodes within the cluster. The replications will run on each of the nodes they are configured on. No automatic distribution of replications is done.
include::partial$block-caveats.adoc[tag=ce-only]

_Related configuration properties_:  {xref-sgw-pg-config-properties}

== Examples of Expected behavior

This section provides examples of expected behavior in differing scenarios and provides a comparison of how behavior differs between {enterprise} and {community}.

Each of these scenarios involves a sync gateway cluster with multiple nodes. There are examples for:

* Homogenous configuration -- see <<homogenous-config>>
* Homogenous configuration with non-replicating node -- see <<homogenous-non-rep-node>>
* Heterogenous configuration -- see <<hetero-config>>
* Adding more nodes -- see <<adding-more-nodes>>
* Failing node -- see <<failing-node>>

[#homogenous-config]
.Homogenous configuration
=====
.Scenario

* The cluster comprises three SGW nodes
* The same sync gateway configuration is applied across all nodes
* All nodes are configured to run _Replication Id 1_

[{tabs}]
====

{enterprise}::
+
--
* SGW automatically designates one of the three nodes to run _Replication Id 1_.
* If a node goes down, SGW elects one of the remaining nodes to continue _Replication Id 1_.
--

{community}::
+
--
SGW runs _Replication  Id 1_ on all nodes in the cluster.
--
====
=====

[#homogenous-non-rep-node]
.Homogenous configuration with non-replicating node
=====

.Scenario
* The cluster comprises three SGW nodes.
* Each node has the same sync gateway configuration, with one exception. The configuration on _Node 3_ has opted out of replication (`sgreplicate_enabled=false`)
* All Three nodes are configured to run _Replication Id 1_.

[{tabs}]
====
{enterprise}::
+
--
* SGW automatically designates either _Node 1_ or _Node 2_ to run the _Replication Id 1_.
* If either _Node 1_ or _Node 2_ fails, SGW elects the non-failing node.
--

{community}::
+
--
* SGW runs _Replication Id 1_ on all nodes in cluster.

* The system ignores the opt-out flag (`sgreplicate_enabled`).
--
====
=====

[#hetero-config]
.Heterogenous configuration
=====
.Scenario
* The cluster comprises three SGW nodes
* Both _Node 1_ and _Node 2_ are configured to run _Replication Id 1_
* _Node 3_ is configured to run _Replication Id 2_ but *not* _Replication Id 1_

[{tabs}]
====
{enterprise}::
+
--

* SGW automatically distributes _Replication Id 1_ and _Replication Id 2_ so that each runs on *one* of _Node 1_, _Node 2_ or _Node 3_, with no node running both replications simultaneously.
* If any node fails whilst running either replication, SGW elects a non-failing node to continue that replication on.
Where two nodes remain the node not running a replication will be chosen.

--

{community}::
+
--
* SGW runs _Replication  Id1_ on _Node 1_ and _Node 2_  in the cluster
* SGW runs _Replication Id 2_ on _Node 3_

Note:

* If _Node 3_ fails, then _Replication Id 2_ will not be continued on either of the remaining nodes as it is not configured on them
* Similarly, if either or both of the other nodes (_Node 1_ and _Node 2_) fails, _Node 3_ will not be a candidate to run the corresponding replication.
--
====
=====

[#adding-more-nodes]
.Adding more nodes
=====

.Scenario
* The cluster comprises a single SGW node
* _Node 1_ is configured to run _Replication Id 1_ and _Replication Id 2_

* LATER . . . _Node 2_ is added to the cluster to run _Replication Id 1_ and _Replication Id 2_.

[{tabs}]
====
{enterprise}::
+
--
* SGW designates _Node 1_ run both _Replication Id 1_ and _Replication Id 2_

* LATER . . . when _Node 2_ is added . . .
** SGW select one of the _Node 1_ replications to run on _Node 2_; let's say it chooses _Replication Id 2_
** SGW stops _Replication Id 2_ on _Node 1_
** SGW starts _Replication Id 2_ on _Node 2_.
--

{community}::
+
--
* SGW designates _Node 1_ to run both _Replication Id 1_ and _Replication Id 2_
* WHEN . . . _Node 2_ is added . . . SGW  designates it to run both _Replication Id 1_ and _Replication Id 2_
--
====
=====

[#failing-node]
.Failing node
=====
.Scenario

* The cluster comprises three SGW nodes with a homogeneous configuration
* All three nodes are configured to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
* LATER . . . _Node 3_ goes down

[{tabs}]
====
{enterprise}::
+
--
SGW automatically distributes the replications, one to each of the nodes
//  to run  _Replication Id 1_, _Id 2_ and _Id 3_ respectively.

* Lets assume the following distribution:
** _Node 1_ runs _Replication Id 1_
** _Node 2_ runs _Replication Id 2_
** _Node 3_ runs _Replication Id 3_

WHEN . . . _Node 3_  goes down . . . SGW elects either _Node 1_ or _Node 2_ to continue running _Replication Id 3_

--

{community}::
+
--
SGW runs all three replications (_Replication Id 1_ , _Replication Id 2_ and _Replication Id 3_) on all three nodes in the cluster (_Node 1_, _Node 2_ and _Node 3_)

WHEN . . . _Node 3_ goes down . . . _Node 1_ and _Node 2_ continue to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
--
====
=====

== Monitoring Node Distribution

Use the __replicationStatus_ endpoint to access information about which replications are running on which nodes -- see: {xref-sgw-ep-admin-api-replication-repstatus}
  |  {xref-sgw-ep-admin-api-replication-repstatus-set}

This information is also collected and available in the log files.




