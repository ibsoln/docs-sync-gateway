= Inter-Cluster Replication
:page-layout: article
:page-status: {release-status-sgw} -- {release-comments-sgw}
:page-edition: Under Development
:page-role:
:description: Inter-cluster replication (Sync Gateway to Sync Gateway) using SG-Replicate 2.0 protocol


include::partial$_std-hdr-sgw.adoc[]

:topic-group: Inter-cluster Replication
:param-related: {xref-sgw-pg-config-properties} | {xref-sgw-pg-rest-api-admin}
:param-abstract: This content provides an overview of Inter-cluster replication, which is replication between Sync Gateway clusters using SG-Replicate 2.0.
include::partial$block-abstract.adoc[]

// include::shared-mobile::partial$_attributes-shared.adoc[]
// include::partial$_attributes-local.adoc[]
// :xref-pfx-sgw: {xref-pfx-sgw}:
// include::partial$_page-index.adoc[]
// include::partial$_glossary-links.adoc[]

ifndef::release-status-sgw[:release-status-sgw!:]
ifeval::["{release-status-sgw}" == "gamma"]
[.pane_frame--orange]
.Author's Notes
--
Create an overview of the Sync Gateway replication process

.points to include

* replicator diagram https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit#heading=h.33gfct4o4z8t


High level intro with links to appropriate details topics

Document replication concepts and use-cases
High-availability sync-gateway cluster replication
Persistent vs Transient replication types
Replication use cases:
mobile cluster replications
cloud-edge/hub-spoke replication

 protocol to enable replication between mobile clusters. That is, for example between a hub and its spoke clusters.
--
endif::[]


// Footnotes
:fn-icr: footnote:icr[As of Sync Gateway version 2.8]
:fn-arch: footnote:arch[This architecture is also known as _ship-to-shore_ or _hub-and-spoke_.]
:fn-icr-ref: footnote:icr[]
:fn-arch-ref: footnote:arch[]


== Overview

Couchbase Sync Gateway supports _{glos-term-inter-cluster-replication}_, {fn-icr} an increasingly important enterprise-level requirement that
supports _{glos-term-cloud-to-edge} synchronization_ use cases, where data changes must be synchronized between a centralized cloud cluster and a large number of edge clusters whilst still enforcing fine grained access control.

Inter-cluster replication provides the ability to run replications between:

* Sync Gateway clusters
* Active mobile clusters.

Sync Gateway's inter-cluster replication feature uses the SG-Replicate 2.0  protocol. It is an integral part of Sync Gateway, acting as the _{glos-term-active-replicator}_, pulling changes from a data source and pushing them to a target.

[[icr-architecture]]
.Inter-cluster replication architecture
====
image::icr-replication-overview.svg[]
====

All documents pass through the target Sync Gateway's _{glos-term-sync-function}_ instance, ensuring that access rules are honored. See: {xref-sgw-pg-adv-sgw-cfg-sync-function} for more on using `sync` functions)

In the architecture diagram (<<icr-architecture>>), any database changes made on either Sync Gateway instance are replicated to the other Sync Gateway instance, in accordance with the replication's configuration -- see {xref-sgw-pg-config-properties-db-replications} for configuration detail.

== Feature Summary

* Replication use cases include (see <<typical-use-cases>>):
** Mobile cluster replications
** Cloud-edge/hub-spoke replication
* {xref-sgw-pg-icr-high-availability} (HA)
* {xref-sgw-pg-icr-delta-sync} option
* Create flexible replications using Sync Gateway's REST API -- see: {xref-sgw-pg-adv-rest-api-client}
* Scalability
* JSON configuration to specify replications
* Supports multiple replications running concurrently
* Supports multiple replication types: _{glos-term-persistent-replication}_ and _{glos-term-transient-replication}_ -- see {xref-sgw-pg-icr-replication-types}
* Leverages the full duplex nature of websockets to support push and pull from a single connection -- see {xref-sgw-pg-config-properties-db-rep-direction}
* Does not store anything persistently
* Stateless -- can be interrupted/restarted anytime without negative side effects
* Can specify which channel(s) to sync
* Supports Primary/Primary and Primary/Secondary topologies

== Use Cases

Cloud-to-edge synchronization::
+
--
In this multi-cloud deployment mode large numbers of multiple edge clusters sync with one or more clusters in cloud data centers. Each edge can operate autonomously without network connectivity to the cloud data centers {fn-arch}.

SG-Replicate 2.0 delivers bi-directional sync, which:

- is scalable
- has high availability
- features automatic and customizable conflict resolution options
- has end-to-end security with fine grained access control.

All very important considerations in this use case.

A typical architecture for this use cases is shown in <<icr-cloud-to-edge>>

[[icr-cloud-to-edge]]
.Cloud-to-edge synchronization
image::icr-cloud-to-edge.svg[]
--

Active-to-active Mobile Synchronization::
+
--
The performant SG-Replicate 2.0 protocol is also ideal where Sync Gateway deployments require data replication between active mobile clusters.

A typical architecture for this use cases is shown in <<icr-active-mobile>>

[[icr-active-mobile]]
.Active-active mobile synchronization
image::icr-active-mobile-sync.svg[]

--

Alternative Methods::
+
--
xref:server:manage:manage-xdcr/xdcr-management-overview.adoc[XDCR] (cross data centre replication) is the Couchbase Server API to replicate between Couchbase Server clusters.

Both XDCR and SG Replicate 2.0 can be used to keep clusters in different data centres in sync.
However, SG Replicate 2.0 was designed specifically for Couchbase Mobile deployments and must be used for replication between mobile clusters.
--

== Constraints

SG-Replicate 2.- will only replicate Sync Gateway databases hosted on versions of Sync Gateway later than March 7, 2014 (commit 50d30eb3d).

High availability::
+
--
In clusters containing multiple Sync Gateway nodes, only _one_ of the Sync Gateways should be configured fas the replicating node.

Configuring multiple Sync Gateways as replicating nodes can substantially increase the amount of duplicate work, and therefore should be avoided.

This limitation means the system cannot be guaranteed as Highly Available. If the replicating Sync Gateway fails or is otherwise removed from the system, then the replications will stop.
--

Conflict Resolution::
+
--
include::{sgw-pg-icr-conflict-resolution}[tag=overview]
--

== Platform Capability

[%autowidth]
|===
| capability |Edition | In CE | In EE

| Replication configured in JSON config file
| CE
| Replication is created on every node it is configured on
| The union of all configured replications is distributed across all config files

| Replication created in REST API
| CE
| Replication is created on node from which the request is made
| The union of all REST API created replications is distributed

| Replication Monitoring using `_replication` and `_activeTasks`
| CE
| Will only return replications local to node on which request was made
| --

| Replication Monitoring using `_replicationStatus`
| CE
| Will only return replications local to node on which request was made
|

| Replication stats reporting (via _expvars)
| CE
|
| Same in CE and EE

| Default Conflict Resolution
| CE
|
| Same in CE and EE

| Out-of-box conflict resolvers (activeWins,passiveWins)
| EE
|
|

| Custom conflict resolution
| EE
|
|

| Delta-Sync
| EE
|
|

| Built-in-High Availability
| EE
| If HA is required, then replicators must be explicitly configured/created on multiple nodes
|

| Filters with Channels
| CE
|
| Same in CE and EE

| Tuneable Parameters
| EE
|
| TBD


|===





== Administration

You can conduct Sync Gateway cluster administration manually should you wish, or circumstances (development environments, production failure or troubleshooting activities) dictate, but in most cases it is likely to be automated. Such automation may be implemented by script, Kubernetes or application server.

Manual::
+
--
Manual operation will typically take place if the cluster is malfunctioning and needs troubleshooting or in demonstration and development environments.
For instance, in the process of debugging why a replication is slow, there would be a need to temporarily stop a replication and subsequently restart when the issue is resolved.  Another case would be if the parameters of the replication need to change.
--

Automated::
+
--
Orchestration::
+
In the case of a decentralized cloud environments managed by an orchestration engine such as K8s, users will likely deploy clusters with a basic configuration. This is because each edge may have different channels and replication requirements.
+
To handle this, the configuration should be sufficient only to connect to the local cluster and to subsequently initialize replications on a per edge basis.
---

Remote Configuration::
A remote configuration facility to manage replications is highly desirable as it would ensure that config changes made to a cluster after launch are persisted on subsequent launches.
+
Under current Sync Gateway architecture the only way to do this is to embed the replication in the configuration file, because transient (`adhoc=true`) replications configured using the REST API are not persisted.

Persisting replications only via config file has practical limitations. For example in cases where a replication is misbehaving causing Sync Gateway to crash (or fail) and must be removed (or updated) so it does not continue to crash (or malfunction) on restart. This may be the case if the replication target changes as a result of an infrastructure change, or filter params change after launch.

Embedding the replication in config file is also not practical in an automated orchestration environment such as K8s, which will auto restart a failing Sync Gateway with same config file, causing repeating fails on restart .

Even outside of K8s, restarting a sync gateway will shed any post-launch changes made by the administrator.

Editing a config file to remove, or update, misconfigured replications means downtime as it  requires taking the Sync Gateway offline for temporary period while the changes are made. In
an automated environment, the system may auto restart another replica to ensure that system has minimal number of servers running. This experience is not frictionless.

Customized::
In lieu of a global cluster-wide admin console for sync gateway (or on a per-node basis for that matter), customers may have to build one on their own. Providing an easy-to-use interface for configuring persistent replications would be highly desirable. Avoids the amount of book keeping that the “admin app” will have to do.
--

Use of `_active_tasks` is deprecated.

=== REST API Admin capabilities:

Use `_replicate` to administrate replications  (`PUT`, `GET`, `DELETE`).

List persistent and transient replications::
+
--
The list includes replications initiated using configuration or REST APi and can be in any state.
--

Stop an active persistent or transient replication::
+
--
For example to offline an edge cluster without waiting for a long replication to complete. The convenience API means only the `replication_id` parameter is required.
--


Reset a persistent replication::
+
--
This is useful to escape a system state where one or more documents have failed to sync but where resuming from previous synced checkpoint would skip over those documents.
--

Update a persistent continuous replication::
+
--

--

Remove a persistent or transient replication::
+
--
Behavior is unchanged, with exception of a convenience API that required only the `replication_id`.
--

== Monitoring Replications

* Use `_replicationStatus` to monitor SG-Replicate 2.0 replications  (`GET`).
* Use `_active_tasks` to monitor SG-Replicate 1.0 replications  (`GET`)
+
The content returned will be the same as that returned by `_replicationStatus` and contains additional content over the SG-Replicate 1.0 version:
+
** direction
** is_persistent
** status (stopped/running).
** last_seq_push
** last_seq_pull



== Filtering



== Authentication




include::partial$block-related-content-icr.adoc[]