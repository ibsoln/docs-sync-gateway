= High-Availability Replication
:page-layout: article
:page-status: {release-status-sgw} -- {release-comments-sgw}
:page-edition: Under Development
:page-role:
:page-content-type: conceptual
:description: High availability and Inter-cluster replication (Sync Gateway to Sync Gateway)
:keywords: replication edge-to-cloud sync 'high availability' edge nosql api synchronization replication

include::partial$_std-hdr-sgw.adoc[]

:topic-group: Inter-cluster Replication
:param-related: {xref-sgw-pg-config-properties} | {xref-sgw-pg-rest-api-admin}
:param-abstract: This content provides an overview of Inter-cluster replication, which is replication between Sync Gateway clusters using SG-Replicate 2.0.
include::partial$_block-abstract.adoc[]

// End of Page Definition Statements

include::shared-mobile::partial$_attributes-shared.adoc[]
include::partial$_attributes-local.adoc[]
:xref-pfx-sgw: {xref-pfx-sgw}:
include::partial$_page-index.adoc[]

ifeval::["{releaseStatus}" == "gamma"]
[.pane__frame--orange]
.Author's Notes
--
Document relevant aspects of _High-Availability Replication_.

Information sources include:

* Ticket: https://issues.couchbase.com/browse/DOC-6494[DOC-6494]
* https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit?ts=5e7cd22f#heading=h.dee3943zlt52

.Points to cover include:

* {empty}

Sync Gateway by default must support  High Availability of a replication.
While a  given replication will only run on one node at any given time but when that node fails, the system will automatically elect another node for replication.

Recommendation : To get HA, Users must be configure at least two nodes to run a given replication.

This will apply to SG-Replicate 2

Impl note: Based on set of replications defined, system will have a node election process/ heartbeat process to detect if a node is up or not)

Note : This requirement is true only for EE. In case of CE, Sync Gateway will behave as it does today. The replicator will run on the nodes that the users designate in their configuration.

In case of multiple  replications, Sync Gateway by default must distribute the set of replications across all available sync gateway nodes (on which the replications are configured). Even if a replication is configured on multiple nodes, a given replication will only run on one node at any given time.

This will apply to SG-Replicate 2

This will provide the following -
Improved throughput by distributing the replications
By ensuring that a replicator only runs on one node even if configured to run on multiple nodes, the system  redundant exchange of data as a result of redundant replications. This will reduce the processing load on the sync gateway and reduce bandwidth usage
This will allow us to deploy a truly homogenous cluster even in SG-replicate based environment where every SGW node in cluster has an identical configuration. This will enable future platform architectural enhancements around mobile manageability .

Impl note: Based on set of replications defined, system will have a node election process/ heartbeat process

Note : This requirement is true only for EE. In case of CE, Sync Gateway will behave as it does today. The replicator will run on the nodes that the users designate in their configuration. This implies that in case of a homogeneous cluster with every node having identical configuration , the replication will run on all nodes.

In EE, replications will get automatically get redistributed as more Sync Gateways are added to the cluster or removed from cluster

Expected behavior
https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit#heading=h.vwjngb7h5lpm
--
endif::[]

[abstract]
This content describes the concept of high-availability replication.

== About High-Availability Replication

[.pane__frame--orange]
.Author's Notes
--
Define HA as a concept and its role in replication
--

Sync Gateway supports High Availability replication. Its
{glos-term-sg-replicate2-protocol} ensures that although a given replication runs on only one node at any given time, if that node fails, the system automatically elects another node for replication. Obviously this requires that the user configure at least two nodes for any given replication.

Good becase it :

* Improves throughput by distributing the replications
By ensuring that a replicator only runs on one node even if configured to run on multiple nodes, the system  redundant exchange of data as a result of redundant replications. This will reduce the processing load on the sync gateway and reduce bandwidth usage

* Enables future platform architectural enhancements around mobile manageability . This will allow us to deploy a truly homogenous cluster even in SG-replicate based environment where every SGW node in cluster has an identical configuration.

== Node Switching

//[{tabs}]
//====
//{enterprise}::
//+
//--

[.pane__frame--orange]
.Author's Notes
--
Explain what this is, why its significant and how it is used.
--

For {enterprise} the node to run on is selected by an election process. It uses a heartbeat check to establish whether a node is up or not. If it isn't it will try the next node in the configured set.
//--

For {community} users the Sync Gateway replicator runs on the node configured by the user.


== Inserted stuff to sort out

== High Availability

There is no change in HA behavior for either SG-Replicate 1.0, or {community} edition, replications.
In both instances, the replicator runs only on the node defined in the configuration.

{enterprise}

Sync Gateway supports highly-available replication. Each replication runs only on one node. If that node fails Sync Gateway will automatically elect an alternate node on which to continue running the replication.

To take advantage of highly available replications, users must configure at least two nodes to run each replication where HA is required.

Where multiple replications are configured Sync Gateway distributes these replications across all the available nodes for which replications are configured. Any given replication will run on only one node at any given point in time.

Replications are automatically redistributed when nodes are added or removed from a cluster.

.HA Behavior in Multiple Scenarios
// ====
|===
| Configured Scenario |Behavior in {enterprise} edition|Behavior in {community} edition

a|* Three SGW nodes in a cluster, with homogeneous configuration.
* All nodes configured to run _Replication Id 1_.

a|* SGW automatically designates one of the three nodes to run _Replication Id 1_.
* If a node goes down, SGW elects a different one of the three nodes.

|SGW runs _Replication  Id 1_ on all nodes in the cluster.

a|* Three SGW nodes in a cluster.
* All Three nodes are configured to run _Replication Id 1_.
* The configuration on _Node 3_ has opted Out (`sgreplicate_enabled=false`) even though it has configured a replication).

a| * SGW automatically designates either _Node 1_ or _Node 2_ to run the _Replication Id 1_.
* If either _Node 1_ or _Node 2_ fails, SGW elects the non-failing node.

|SGW runs _Replication Id 1_ on all nodes in cluster.

The system ignores the opt-out flag (`sgreplicate_enabled`).

a| * Three SGW nodes in a cluster.
* _Node 1_ and _Node 2_  are configured to run _Replication Id 1_.
* _Node 3_ is configured to run _Replication Id 2_.

a|* SGW automatically distributes replications such that each replication runs on one of _Node 1_, _Node 2_ or _Node 3_.
* If any nodes fail, SGW elects a non-failing node to run replication on.

a| * SGW runs _Replication  Id1_ on _Node 1_ and _Node 2_  in the cluster.
* SGW runs _Replication Id 2_ on _Node 3_.

a|* A single SGW node in a cluster.
* _Node 1_ is configured to run _Replication Id 1_ and _Replication Id 2_.
* LATER, _Node 2_ is added to the cluster to run _Replication Id 1_ and _Replication Id 2_.


a| * SGW designates _Node 1_ run _Replication Id 1_ and _Replication Id 2_.
* LATER, when _Node 2_ is added, SGW designates either _Replication Id 1_ or _Replication Id 2_ to run on _Node 2_ (and stops the corresponding replication on _Node 1_).

a|* SGW designates _Node 1_ to run _Replication Id 1_ and _Replication Id 2_
* LATER, when _Node 2_ is added SGW  designates _Node 2_ to runs _Replication Id 1_ and _Replication Id 2_

a|* Three SGW nodes in cluster with a homogeneous configuration.
* All three nodes are configured to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
* LATER, _Node 3_ goes down

a|* SGW automatically designates one of the nodes to run _Replication Id 1_, _Id 2_ and _Id 3_ respectively.
* I we assume the following distribution:
** _Node 1_ runs _Replication Id 1_
** _Node 2_ runs _Replication Id 2_
** _Node 3_ runs _Replication Id 3_
* LATER, when _Node 3_  goes down, SGW elects either _Node 1_ or _Node 2_ to run _Replication Id 3_.

a|* SGW runs replication  Id1 , Id2 and Id3 on all nodes in cluster
* LATER, when _Node 3_ goes down, _Node 1_ and _Node 2_ continue to run _Replication Id 1_, Id2 and Id3

|===
// ====





== Configuration Requirements
[.pane__frame--orange]
.Author's Notes
--
Refactor and supplement below as necessary. Include links to {xref-sgw-pg-config-properties}
--

* Configure multiple nodes for each replication required to be highly-available

* {empty}
+
--
In case of multiple  replications, Sync Gateway by default must distribute the set of replications across all available sync gateway nodes (on which the replications are configured). Even if a replication is configured on multiple nodes, a given replication will only run on one node at any given time.


This will apply to SG-Replicate 2

This will provide the following -
Improved throughput by distributing the replications
By ensuring that a replicator only runs on one node even if configured to run on multiple nodes, the system  redundant exchange of data as a result of redundant replications. This will reduce the processing load on the sync gateway and reduce bandwidth usage
This will allow us to deploy a truly homogenous cluster even in SG-replicate based environment where every SGW node in cluster has an identical configuration. This will enable future platform architectural enhancements around mobile manageability .

Impl note: Based on set of replications defined, system will have a node election process/ heartbeat process

Note : This requirement is true only for EE. In case of CE, Sync Gateway will behave as it does today. The replicator will run on the nodes that the users designate in their configuration. This implies that in case of a homogeneous cluster with every node having identical configuration , the replication will run on all nodes.
--
* {empty}
+
[.edition]
image::{enterprise}[]
+
--
In EE, replications will get automatically get redistributed as more Sync Gateways are added to the cluster or removed from cluster
--

== Examples of Expected behavior

[.pane__frame--orange]
.Author's Notes
--
Insert content based on this table here.
https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit#heading=h.vwjngb7h5lpm
--

include::partial$block-related-content-icr.adoc[]