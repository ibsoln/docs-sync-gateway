= High-Availability Replication
:page-partial:
:page-layout: article
:page-status: {release-status-sgw} -- {release-comments-sgw}
:page-edition: enterprise
:page-role:
:page-content-type: conceptual
:description: High availability and Inter-cluster replication (Sync Gateway to Sync Gateway)
:keywords: replication edge-to-cloud sync 'high availability' edge nosql api synchronization replication

include::partial$_std-hdr-sgw.adoc[]
include::partial$block-authors-notes.adoc[tag=wip]
:topic-group: Inter-cluster Replication
:param-related: {xref-sgw-pg-config-properties} | {xref-sgw-pg-rest-api-admin}
:param-abstract: This content provides an overview of Inter-cluster replication high-availability
include::partial$block-abstract.adoc[]

// End of Page Definition Statements

include::shared-mobile::partial$_attributes-shared.adoc[]
include::partial$_attributes-local.adoc[]
:xref-pfx-sgw: {xref-pfx-sgw}:
include::partial$_page-index.adoc[]

ifeval::["{releaseStatus}" == "gamma"]
[.pane__frame--orange]
.Author's Notes
--
Document relevant aspects of _High-Availability Replication_.

Information sources include:

* Ticket: https://issues.couchbase.com/browse/DOC-6494[DOC-6494]
* https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit?ts=5e7cd22f#heading=h.dee3943zlt52

.Points to cover include:

* {empty}

Sync Gateway by default must support  High Availability of a replication.
While a  given replication will only run on one node at any given time but when that node fails, the system will automatically elect another node for replication.

Recommendation : To get HA, Users must be configure at least two nodes to run a given replication.

This will apply to SG-Replicate 2

Impl note: Based on set of replications defined, system will have a node election process/ heartbeat process to detect if a node is up or not)

Note : This requirement is true only for EE. In case of CE, Sync Gateway will behave as it does today. The replicator will run on the nodes that the users designate in their configuration.

In case of multiple  replications, Sync Gateway by default must distribute the set of replications across all available sync gateway nodes (on which the replications are configured). Even if a replication is configured on multiple nodes, a given replication will only run on one node at any given time.

This will apply to SG-Replicate 2

This will provide the following -
Improved throughput by distributing the replications
By ensuring that a replicator only runs on one node even if configured to run on multiple nodes, the system  redundant exchange of data as a result of redundant replications. This will reduce the processing load on the sync gateway and reduce bandwidth usage
This will allow us to deploy a truly homogenous cluster even in SG-replicate based environment where every SGW node in cluster has an identical configuration. This will enable future platform architectural enhancements around mobile manageability .

Impl note: Based on set of replications defined, system will have a node election process/ heartbeat process

Note : This requirement is true only for EE. In case of CE, Sync Gateway will behave as it does today. The replicator will run on the nodes that the users designate in their configuration. This implies that in case of a homogeneous cluster with every node having identical configuration , the replication will run on all nodes.

In EE, replications will get automatically get redistributed as more Sync Gateways are added to the cluster or removed from cluster

Expected behavior
https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit#heading=h.vwjngb7h5lpm
--
endif::[]

[abstract]


== Introduction

include::partial$block-caveats.adoc[tag=icr-sgr1-community]


== About High-Availability Replication

// tag::overview[]
Sync Gateway's inter-cluster replications support High Availability (HA).

The SG-Replicate 2.0 protocol ensures that any given replication runs on only one node at any given time.
However, if that node fails the system automatically resumes the replication on one of the alternative nodes, providing such nodes have been configured.

Replications are automatically redistributed as the number of available nodes in the cluster changes.
// end::overview[]

_Action_: To make a replication a candidate for HA the user must configure replications required to run in HA mode on at least two Sync Gateway nodes.

== Node Distribution
Sync Gateway minimizes the processing load and reduces bandwidth usage by ensuring that a replicator only runs on one node at any given time -- even if configured to run on multiple nodes.
This is because it avoids the redundant exchange of data arising from duplicate replication.

Where multiple replications are configured on multiple nodes, Sync Gateway automatically distributes these replications across all the available nodes for which replications are configured.

Sync Gateway balances, as equally as possible, the number of replications running on each node.

As the number of available nodes with the cluster changes, the replications are redistributed to maintain an optimal balance across the cluster.

// Sync Gateway selects the node to run a replication on is selected by an election process. It uses a heartbeat check to establish whether a node is available or not.
// If it isn't it will try the next node in the configured set.

// Monitoring the node heartbeats is also the trigger for the re-balancing of running replications as it






== Configuration Requirements

[.pane__frame--orange]
.Author's Notes
--
Include links to {xref-sgw-pg-config-properties}
Configure multiple nodes for each replication required to be highly-available -- Examples linked tp scenarios below?

--


== Examples of Expected behavior

.HA Behavior in Multiple Scenarios
=====
[{tabs}]
====

{enterprise}::
+
--

|===
| Configured Scenario |Behavior in {enterprise}

a|* Three SGW nodes in a cluster, with homogeneous configuration.
* All nodes configured to run _Replication Id 1_.

a|* SGW automatically designates one of the three nodes to run _Replication Id 1_.
* If a node goes down, SGW elects a different one of the three nodes.

// |SGW runs _Replication  Id 1_ on all nodes in the cluster.

a|* Three SGW nodes in a cluster.
* All Three nodes are configured to run _Replication Id 1_.
* The configuration on _Node 3_ has opted Out (`sgreplicate_enabled=false`) even though it has configured a replication).

a| * SGW automatically designates either _Node 1_ or _Node 2_ to run the _Replication Id 1_.
* If either _Node 1_ or _Node 2_ fails, SGW elects the non-failing node.

// |SGW runs _Replication Id 1_ on all nodes in cluster.

// The system ignores the opt-out flag (`sgreplicate_enabled`).

a| * Three SGW nodes in a cluster.
* _Node 1_ and _Node 2_  are configured to run _Replication Id 1_.
* _Node 3_ is configured to run _Replication Id 2_.

a|* SGW automatically distributes replications such that each replication runs on one of _Node 1_, _Node 2_ or _Node 3_.
* If any nodes fail, SGW elects a non-failing node to run replication on.

// a| * SGW runs _Replication  Id1_ on _Node 1_ and _Node 2_  in the cluster.
// * SGW runs _Replication Id 2_ on _Node 3_.

a|* A single SGW node in a cluster.
* _Node 1_ is configured to run _Replication Id 1_ and _Replication Id 2_.
* LATER, _Node 2_ is added to the cluster to run _Replication Id 1_ and _Replication Id 2_.


a| * SGW designates _Node 1_ run _Replication Id 1_ and _Replication Id 2_.
* LATER, when _Node 2_ is added, SGW designates either _Replication Id 1_ or _Replication Id 2_ to run on _Node 2_ (and stops the corresponding replication on _Node 1_).

// a|* SGW designates _Node 1_ to run _Replication Id 1_ and _Replication Id 2_
// * LATER, when _Node 2_ is added SGW  designates _Node 2_ to runs _Replication Id 1_ and _Replication Id 2_

a|* Three SGW nodes in cluster with a homogeneous configuration.
* All three nodes are configured to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
* LATER, _Node 3_ goes down

a|* SGW automatically designates one of the nodes to run _Replication Id 1_, _Id 2_ and _Id 3_ respectively.
* I we assume the following distribution:
** _Node 1_ runs _Replication Id 1_
** _Node 2_ runs _Replication Id 2_
** _Node 3_ runs _Replication Id 3_
* LATER, when _Node 3_  goes down, SGW elects either _Node 1_ or _Node 2_ to run _Replication Id 3_.

// a|* SGW runs replication  Id1 , Id2 and Id3 on all nodes in cluster
// * LATER, when _Node 3_ goes down, _Node 1_ and _Node 2_ continue to run _Replication Id 1_, Id2 and Id3

|===
--

{community}::
+
--
|===
| Configured Scenario |Behavior in {community}

a|* Three SGW nodes in a cluster, with homogeneous configuration.
* All nodes configured to run _Replication Id 1_.

// a|* SGW automatically designates one of the three nodes to run _Replication Id 1_.
// * If a node goes down, SGW elects a different one of the three nodes.

|SGW runs _Replication  Id 1_ on all nodes in the cluster.

a|* Three SGW nodes in a cluster.
* All Three nodes are configured to run _Replication Id 1_.
* The configuration on _Node 3_ has opted Out (`sgreplicate_enabled=false`) even though it has configured a replication).

// a| * SGW automatically designates either _Node 1_ or _Node 2_ to run the _Replication Id 1_.
// * If either _Node 1_ or _Node 2_ fails, SGW elects the non-failing node.

|SGW runs _Replication Id 1_ on all nodes in cluster.

The system ignores the opt-out flag (`sgreplicate_enabled`).

a| * Three SGW nodes in a cluster.
* _Node 1_ and _Node 2_  are configured to run _Replication Id 1_.
* _Node 3_ is configured to run _Replication Id 2_.

// a|* SGW automatically distributes replications such that each replication runs on one of _Node 1_, _Node 2_ or _Node 3_.
// * If any nodes fail, SGW elects a non-failing node to run replication on.

a| * SGW runs _Replication  Id1_ on _Node 1_ and _Node 2_  in the cluster.
* SGW runs _Replication Id 2_ on _Node 3_.

a|* A single SGW node in a cluster.
* _Node 1_ is configured to run _Replication Id 1_ and _Replication Id 2_.
* LATER, _Node 2_ is added to the cluster to run _Replication Id 1_ and _Replication Id 2_.


// a| * SGW designates _Node 1_ run _Replication Id 1_ and _Replication Id 2_.
// * LATER, when _Node 2_ is added, SGW designates either _Replication Id 1_ or _Replication Id 2_ to run on _Node 2_ (and stops the corresponding replication on _Node 1_).

a|* SGW designates _Node 1_ to run _Replication Id 1_ and _Replication Id 2_
* LATER, when _Node 2_ is added SGW  designates _Node 2_ to runs _Replication Id 1_ and _Replication Id 2_

a|* Three SGW nodes in cluster with a homogeneous configuration.
* All three nodes are configured to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
* LATER, _Node 3_ goes down

// a|* SGW automatically designates one of the nodes to run _Replication Id 1_, _Id 2_ and _Id 3_ respectively.
// * I we assume the following distribution:
// ** _Node 1_ runs _Replication Id 1_
// ** _Node 2_ runs _Replication Id 2_
// ** _Node 3_ runs _Replication Id 3_
// * LATER, when _Node 3_  goes down, SGW elects either _Node 1_ or _Node 2_ to run _Replication Id 3_.

a|* SGW runs replication  Id1 , Id2 and Id3 on all nodes in cluster
* LATER, when _Node 3_ goes down, _Node 1_ and _Node 2_ continue to run _Replication Id 1_, Id2 and Id3

|===

--

====
=====

== Monitoring Node Distribution

Use the __replicationStatus_ endpoint to access information about which replications are running on which nodes -- see: {xref-sgw-endpoint-admin-api-replication-repstatus}
  |  {xref-sgw-endpoint-admin-api-replication-repstatus-set}

This information is also collected and available in the log files.




include::partial$block-related-content-icr.adoc[]